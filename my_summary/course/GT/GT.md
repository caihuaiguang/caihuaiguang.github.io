 第一部分：计算博弈的基础知识
• 博弈论的基本假设

博弈论的三个基本假设
• 参与的局中人都是理性的（rational）：自己为自己的行为负责，前后一致、至始至终遵循自己的决策，考虑对方的决策，考虑不确定性等。
• 参与的局中人都是聪明的（intelligent）：博弈中能够洞悉一切，对博弈的变化永久记忆，永不遗忘；对整个博弈局势了如指掌，并作出完整的判断；双方都知道关于博弈的相关信息。
• 参与的局中人都是经济的（economical）：每个参与的局中人都是追求博弈过程中的个人效用最大化，效用通常使用效用函数来表示。

• 博弈的基本要素

描述一个博弈模型，至少需要以下三个要素：
• 局中人或者参与人：是博弈中的行为主体，可能是自然人，也可能是企业、团体、特定群体，甚至可以是虚拟的参与人、无形的自然。有时候为了分析方便，参与人每个可能信息状态都可以看到代理人。
• 策略空间：参与人𝑖可选择的行动策略集合记为𝑆𝑖，则每一个选择的策略s𝑖 ∈ S𝑖，𝐬 = 𝑠𝑖, … , 𝑠𝑛 称为n个参与人的策略组合（strategy profile），所有参与人的策略集合组合就构成了博弈的策略空间𝐒 = ×1≤𝑖≤𝑛 𝑆𝑖。
• 损益函数：或者称为收益函数、支付函数等，是博弈的局中人最关心的。

• 纯策略与混合策略的定义与表示方法

参与人在给定情况下只选择一种特定的行动，我们就称该策略为纯（Pure）策略
参与人在给定情况下以某种概率分布随机选择不同的行动，就称该策略为混合（ Mixed ）策略

• 常见博弈类型与举例描述

按照参与人是否合作：
• 非合作博弈：博弈的每个个体参与者都独立地以自己的个人理性一直进行决策，最优化个人的效益。
• 合作博弈：博弈的一些参与者以同盟、合作的方式进行的博弈，博弈活动就是不同集团之间的对抗。 

按照行动的先后次序
• 静态博弈：是指博弈过程中参与人行动时预先不知晓其他参与人行动的博弈。
• 动态博弈：是指参与人行动有先后顺序，且后行动者能够知晓先行动者所选择行动的博弈。
• 随机博弈：是指参与人行动既有同时发生、又有顺序发生的博弈；进行具有状态概率转移的博弈过程。
• 举例
• 静态博弈：囚徒困境、石头剪刀布等
• 动态博弈：象棋围棋、商品拍卖、军备竞赛等
• 随机博弈：实时策略游戏、真实战争等

根据参与人的多少
• 单人博弈：个人在面临多个可选策略时的思考过程，“跟自己玩并后果自负”；
• 双人博弈：博弈的参与者包含两个不同个体；
• 多人博弈：博弈的参与者包含多个不同个体；
• 举例
• 单人博弈：是否选修博弈论课，是否退选博弈论课，是否找女朋友，AlphaZero模型自我博弈等
• 双人博弈：囚徒困境、围棋、象棋等
• 多人博弈：王者荣耀5v5等

根据博弈的次数
• 单次博弈：博弈的过程只进行一次
• 有限多次博弈：博弈的过程包含多次，是一类特殊的扩展形式博弈
• 无限重复博弈：博弈的过程重复多次，每个参与者会考虑自己当期的行为对其他参与者未来行为的影响

按照参与人之间状态信息的知晓程度
• 完全信息博弈：博弈的所有参与者都对博弈各方的各种情况下的收益完全知晓；
• 不完全信息博弈：博弈的参与者对博弈各方的各种情况下的收益不完全知晓；
• 完美信息博弈：博弈的所有参与者都完全知晓他行动前的博弈过程和历史；
• 不完美信息博弈：不是所有的博弈参与者都完全知晓他行动前的博弈过程和历史；
大家举例：

根据博弈的结果分类
• 零和博弈：指参与博弈的各方，在严格竞争下，一方
的收益必然意味着另一方的损失，博弈各方的收益和
损失相加总和永远为“零”，双方不存在合作的可能。
• 负和博弈：指双方竞争结果造成博弈结果总和为负数。
它既包括一种两败俱伤的情况，这种情况下结果双方
都有不同程度的损失；他也包括另一种“胜者”取得
的利益小于“败者”承受的损失的博弈。
• 正和博弈：指博弈双方的利益都有所增加，或者至少
是一方的利益增加，而另一方的利益不受损害，因而
整个社会的利益有所增加，对应合作博弈。


• 博弈的矩阵式表示和扩展式表示互相转化的方法

矩阵式转化为扩展式： 利用信息集来表示动作同时进行
扩展式转化为矩阵式：首先写出所有玩家的纯策略集合，集合大小确定了矩阵每一维度的大小，然后用收益函数进行填空

• 信息集，完全信息，完美信息，共同知识等概念

信息集（Information Set，博弈论中的重要概念）：
1）集合中的每个节点都是同一个参与人进行决策；2）参与人知道博弈进入该集合，但是不知道自己具体在哪一个节点；3）每一个信息集中节点的
可选动作都相同

不完美信息（Imperfect Information）：一些参与者观测不到其他参与者的动作选择
完美信息（Perfect Information） ：参与人对其他参与人（包括虚拟参与人）的行动信息有准确的了解，即每个信息集只有一个节点。
完全信息：参与人对博弈的支付函数信息有准确的了解。
共同知识（Common Knowledge）：对于某个事实，如果每个参与者知道该事实，并且每个参与者知道每个参
与者知道该事实，如此循环下去，那么该事实就是所有参与者通享的共同知识。

• 均衡，帕累托占优，纳什均衡的基本概念

均衡：是所有参与人的最优策略的组合
帕累托最优（ Pareto Optimality ） ：资源分配的一种理想状态，不存在使别人利益不受损的情况下改进个别人的利益。
帕累托占优（Pareto Domination）：策略组合 𝒔 = (𝑠𝑖, … , 𝑠𝑛) 帕累托占优𝒔′= (𝑠𝑖′, … , 𝑠𝑛′)：如果对于任何玩家𝑖 ∈ 𝑁来说， 𝑢𝑖(𝒔) ≥ 𝑢𝑖(𝒔′)
纳什均衡（Nash Equilibrium）：在一个策略组合中，所有的参与者面临这样的一种情况，当其他人不改变策略时，他此时的策略是最好的。



第二部分：四种不同类型的博弈
完全信息静态博弈：纳什均衡
完全信息动态博弈：子博弈精练纳什均衡
不完全信息静态博弈：贝叶斯均衡
不完全信息动态博弈：精炼贝叶斯均衡

• 占优策略均衡的定义与求取

给定策略式博弈，如果对于每个参与人𝑖，𝑠𝑖∗ ∈ 𝑆𝑖都是𝑖的强优势策略，
即：𝜇𝑖(𝑠𝑖∗, 𝑠−𝑖) > 𝜇𝑖(𝑠𝑖, 𝑠−𝑖), ∀𝑠−𝑖 ∈ 𝑆−𝑖, ∀𝑠𝑖 ∈ 𝑆𝑖 ∧ 𝑠𝑖 ≠ 𝑠𝑖∗
那么策略组合𝑠∗= (𝑠1∗, ⋯ , 𝑠𝑛∗) 称为该博弈的占优策略均衡。

求取：重复剔除劣策略
• 首先找到某个参与人劣策略
• 剔除这个劣策略得到新博弈
• 对新博弈继续重复上述过程

• 纯策略纳什均衡：库诺特寡头竞争模型等
• 混合策略纳什均衡：社会福利博弈，税收监督博弈等
写出效用函数，求导为0
给定对方选择概率，我方选两个动作得到的效益相同，得到对方的最优动作。

• 扩展式博弈中纳什均衡的两种求解方法：转换法和递归法
转换法将扩展式转换为策略式，再去求解纳什均衡，可能会多出不合理的均衡点。

• 子博弈精炼纳什均衡定义与求取：斯坦尔伯格寡头竞争模型等
子博弈精炼纳什均衡：一个策略组合是子博弈精炼纳什均衡，
当且仅当它在每一个子博弈（包括原博弈）上都构成一个纳什均衡。

逆向递归法给出的完全信息动态博弈的均衡，被称为子博弈精炼纳什均衡

• 海萨尼转换

将不完全博弈转换为完全不完美信息博弈
•方法：通过引入一个虚拟的参与人——“自然”（Nature），
来对博弈中的相关局中人的不确定性因素进行“行动”，得到其确定性结果（特性，type），
然后告知相关局中人，使得博弈继续分析下去。

海萨尼转换的条件：海萨尼公理：关于博弈参与人的类型分布函数𝑝(𝜃1, ⋯ , 𝜃𝑛) 是所有参与人的共同知识。--------------------
参与人在其开始行动前拥有的私有信息称为参与人的类型
（type），一些常见博弈中的私有信息示例如下：进入市场已有企业的成本

• 贝叶斯纳什均衡定义与应用：不完全信息双寡头竞争模型等---------------------------
对于给定的博弈，若信念的一致性条件得以满足，我们就把它称为贝叶斯博弈。

• 贝叶斯博弈与混合策略均衡之间的关系
完全信息情况下的混合策略均衡可以解释为不完全信息情况下的纯策略均衡的极限-------------------

• 信号传递博弈，分离均衡，混同均衡，序贯均衡，颤抖手均衡
的概念

信号传递博弈：两个参与人，参与人1称为信号的发送者，参
与人2称为信号的接收者；参与人1的类型是私人信息，
参与人2的类型是公共信息。

信号传递博弈可能的精炼贝叶斯均衡的三类均衡:
分离均衡：不同类型的发送者以确定性的概率选择发送不同的信号。在分离均衡下，信号准确地揭示出类型。
混同均衡：不同类型的发送者选择相同的信号/没有任何类型选择其他类型的信号。接收者不修正先验概率。

序贯均衡：
基本思想是在子博弈精炼纳什均衡或贝叶斯精炼纳什均衡的基础上增加了一个新的要求：
在博弈到达的每一个信息集上，参与人的行动必须由某种有关之前发生的事情的信念“合理化”。

颤抖手均衡：
在任何博弈中，任何参与人都有一定可能性犯错误，一个策略组合，
只有当它在允许所有参与人都可能犯错时仍是每一个参与人的最优策略组合的时候，才是一个均衡。

第三部分：算法博弈论相关内容
• 算法博弈论主要研究内容

算法化机制设计（Algorithmic Mechanism Design）
• 通俗来讲就是设计博弈的规则，使得各个参与者在追求个人利益的同时能够达到设计者所设定的目标

均衡的低效率性（Inefficiency of Equilibria）
• 研究复杂博弈中均衡解和全局最优解的关系，可以理解成理想化计划经济和自由市场经济的对比，如果有一件事情，需要多人合作，在不靠虑个人得失的情况下，假设最好能产出A ；
但如果让这些人自己玩，最后收敛到一个均衡，产出了 B，A与B之间的大小关系反映了均衡与最优之间的差距

均衡计算复杂度（Complexity of Finding Equilibria）
• 纳什只证明了均衡的存在性，但没有说如何能找到；研究如何通过算法求解博弈的均衡，如果无法在多项式时间内求解，证明此类问题是难的

• 第一价格拍卖的均衡策略与分析
2个人的话不完全信息静态博弈的均衡是：每个投标人出价是其实际价值的一半

• 第二价格拍卖的占优策略与分析
在一个第二价格拍卖中，每一个参与者𝑖都有占优策略：设定他的出价𝑏𝑖为自己对于拍卖品的私有评估值𝑣i

• 最优拍卖的基本流程与应用难点
在一个拍卖中，如果对于每一个竞拍者，真实报价总是占优策略，并且真实报价总是得到非负的收益，那么我们就称该拍卖具备DSIC特性

完美拍卖：满足DSIC、社会福利最大化、可以高效计算
设计完美拍卖两步走：
• 1.假设真实报价，设计分配规则满足：社会福利最大化并可高效计算
• 2.设计支付规则使得DSIC特性成立，也就是真实报价是占优策略

难点：分配规则无法高效计算：背包问题是NP难问题，没有多项式复杂度的算法

• Pigou网络中的纳什均衡策略与最优策略以及PoA的计算方法。
Pigou网络：
纳什均衡策略：都选下路径，平均通勤时间1小时
最优策略：上下路各一半，平均通勤时间3/4小时
PoA =4/3，均衡策略的代价/最优策略的代价>=1
• 对自私路由网络的改进策略：网络预留，技术升级等

• 第四部分：计算博弈的热点应用
• 小游戏强化学习问题与博弈问题之间的联系
强化学习算是一个单人序贯博弈，奖励r对应阶段效用函数，回报G就是多阶段期望效用，折扣因子对应贴现因子什么的，
对于很多强化学习问题，比如德州扑克，博弈均衡策略就是强化学习中的最优策略

• 小游戏AI的学习思路和DQN算法流程
• Double DQN, Dueling DQN, Prioritized Experience Replay改进点
DDQN合理利用DQN中的当前Q网络和目标Q网络 ，用当前Q网络𝑄𝜑选择动作，再用目标Q网络𝑄𝜑′估计Q值.
Prioritized Experience Replay：带优先级的经验回放；重要性采样来解决“有偏”问题：既保证每个样本被选到的概率是不同的（从而提升训练速度），又保证它们对梯度下降的影响是相同的（从而保证收敛效果）。
Dueling DQN：将𝑄函数分解成价值函数𝑉和优势函数𝐴

• AlphaGo算法流程
看图
基于价值网络和策略网络的蒙特卡洛树搜索
• AlphaGo Zero的主要创新点
自我博弈驱动的增强版强化学习过程和模型
舍弃快速走子网络和改进MCTS树搜索过程
价值网络和策略网络使用一个模型进行训练
使用残差结构的神经网络结构大幅提升性能

• AlphaZero的主要创新点
• AlphaGo Zero模型只估计了赢和输两种博弈结果，
AlphaZero同时估计赢、输和平三种结果，以及其他潜在
上的结果
• 考虑到围棋棋局的对称性，AlphaGo Zero在训练数据增
强和深度网络评估时都对一个棋局数据进行八倍增强
（上下翻转、左右翻转、90度旋转）
• AlphaGo Zero的自我博弈数据由之前所有模型中最好的
一个模型产生，AlphaZero只保存当前不断更新的一个模
型，不需要额外的模型评估和选择过程
• AlphaGo Zero使用贝叶斯优化调节超参数，AlphaZero将
这一方法用于不同的游戏，唯一的差别是在先验策略中
增加了噪声来保证模型的探索能力

• CFR和CFR+ 算法的基本思路
CFR是一种自我博弈迭代式求解算法：
每一轮根据当前策略计算遗憾值、然后求下一轮的策略，
最终的平均策略收敛到纳什均衡
具体过程：
• CFR算法的过程总结
• 初始化双方策略（随机）、累积遗憾值（0）
• 双方使用当前策略进行博弈，计算每个信息上每个动作的遗憾值，然后更新累积遗憾值 
• 基于累积遗憾值利用regret matching算法计算下一轮策略
• 不断迭代，最终求取每一轮的平均策略
可以通过递归的形式进行实现
• 最终产生的策略是一个巨大的表格
• 表格的索引是信息集，内容是该信息集下动作的概率分布
• 虽然理论上只有对两人零和博弈才能收敛到纳什均衡，但是对
于一些非两人零和博弈，实验中发现也能产生非常强的策略

遗憾最小化与纳什均衡关系定理：对于两人零和博弈，在时刻𝑇如果两者的𝑅𝑖𝑇都小于𝜖，那么(𝜎0𝑇, 𝜎1𝑇)是一个2𝜖纳什均衡

• DeepStack 的求解思路与主要贡献点
将原游戏的信息集约简，用CFR求得约简博弈的纳什均衡组合，再求得原游戏的（近似）纳什均衡策略组合
在需要做决定时，对于当前的“子博弈”重新实时计算（重建）一个策略，这叫做Re-Solving

第一个击败两人无限注德扑专业选手的AI
整个过程中没有使用人类比赛数据
算法是通用的，对于其他一些非完美信息博弈也有借鉴意义

• Libratus的主要模块与贡献点
完整游戏进行约简：将完整游戏约简成一个相对简单的博弈，根据CFR算法求解得到一个粗略的策略，该策略称之为蓝图策略（Blueprint Strategy）
嵌套安全子博弈求解：在蓝图策略基础上，基于当前的牌面和比赛情况，构建一个全新的、更精细的子博弈，并对这个子博弈的策略进行实时求解
自我提升：随着比赛的进行，利用对手的动作填补蓝图策略中缺失的分支，并为这些分支计算策略 

• 基于约简的策略已经可以进行实战，并能够击败普通玩家。后两个模块是对第一个模块的修补，用于降低因为约简而带来的影响
• 第二个模块用于处理当对手的动作不属于约简动作时的情况
• 第三个模块通过比赛中对手的动作，不断丰富步骤一得到的蓝图策略

第一个击败两人无限注德扑顶级选手的AI
将安全子博弈求解引入到大规模不完美信息博弈求解
算法没有使用任何深度学习模型

• Pluribus算法的主要流程和存在的问题
第一个将AI应用到多人零和博弈中与人类职业选手对战，并取得战胜人类玩家的战绩
将自我博弈应用到多人博弈环境中，并离线产生蓝图策略
多人零和博弈的均衡解问题仍然悬而未决

• 多智能体强化学习中马尔可夫博弈的形式化表示及相关描述
马尔科夫博弈，又称随机博弈（Stochastic Games），是描述多智能体学习的理论框架
< 𝑁, 𝑆, 𝐴, 𝑅, 𝑃, 𝛾 > 六元组

• 自博弈，虚拟博弈，元博弈等概念解释、

虚拟博弈（Fictitious Play）
• 记录对手的历史动作信息→求取对手采取不同动作的概率，然后按照对手的动作概率选取最优反应，不断迭代
在两人零和博弈中，虚拟博弈收敛到纳什均衡

元博弈，Meta-Games
• 对于真实世界的复杂博弈，进行动作级别的分析是行不通的
• 需要进行策略层面的分析

• AlphaStar, OpenAI Five, 觉悟AI 等的设计流程

